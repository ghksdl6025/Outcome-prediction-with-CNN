{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying embedding layer\n",
    "\n",
    "Let's apply embedding layer before linear layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils as torch_utils\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader,Dataset,WeightedRandomSampler\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "# !nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct custom dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Customdataset(Dataset):\n",
    "    def __init__(self,x_data,y_data,transform=None):\n",
    "        '''\n",
    "        Call stored dataset\n",
    "        \n",
    "        Params\n",
    "        second: Ellapsed second from the beginning of events \n",
    "        encoding_type: Encoding method for outcomeprediction ex) Static, last_state, aggregation, etc.\n",
    "        '''\n",
    "        \n",
    "            \n",
    "        # Transforms\n",
    "        self.y_data=np.array(y_data)\n",
    "        self.x_data=x_data\n",
    "    def __len__(self):\n",
    "        return len(self.x_data)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        \n",
    "        # Convert x and y data to torch flaot tensor\n",
    "        x = self.x_data[idx]\n",
    "        y = self.y_data[idx]\n",
    "        return x,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To equalize size of each input data, delete columns that doesn't exist across all cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding_type = 'indexbase'\n",
    "prefix = 5\n",
    "input_data = pd.read_csv('../data/bpic2011/'+encoding_type+'_prefix'+str(prefix)+'.csv')\n",
    "input_data =input_data.drop(['Case ID'],axis=1)\n",
    "\n",
    "y_data = [int(y) for y in list(input_data['Label'])]\n",
    "input_data = input_data.drop(['Label'],axis=1)\n",
    "\n",
    "x_data = input_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So,  \n",
    "Event log: BPIC 2011  \n",
    "prefix length: 5  \n",
    "Number of event categorical attributes: 8  \n",
    "Number of event categorical attributes columns in pre-preprocessed: 839\n",
    "Number of event continuous attributes: 2  \n",
    "Number of items in single event (including embedding_dim_size,5): 10 (42,âˆµ 8\\*5+2)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_cat =['Activity','Section','Specialism code','Producer code','org:group','Timemonth','Timeweekday','Timehour']\n",
    "event_con =['Duration','Cumduration']\n",
    "event_cat_col=[]\n",
    "event_con_col=[]\n",
    "\n",
    "for col in x_data.columns.values:\n",
    "    for e_col in event_cat:\n",
    "        if e_col in col:\n",
    "            event_cat_col.append(col)\n",
    "    for e_col in event_con:\n",
    "        if e_col in col:\n",
    "            event_con_col.append(col)\n",
    "        \n",
    "only_event_cat = x_data.loc[:,event_cat_col]\n",
    "vocab = {word: i+2 for i, word in enumerate(event_cat_col)}\n",
    "vocab['<boc>'] =0\n",
    "vocab['<eoc>'] =1\n",
    "\n",
    "embedding_layer = nn.Embedding(num_embeddings = len(vocab), \n",
    "                               embedding_dim = 6,\n",
    "                               padding_idx = 1)\n",
    "\n",
    "event_cat_col = sorted(event_cat_col,key=lambda x:x.split('_')[1])\n",
    "event_con_col = sorted(event_con_col,key=lambda x:x.split('_')[1])\n",
    "\n",
    "nx_data = []\n",
    "for row in range(len(x_data)):\n",
    "    row_train=[]\n",
    "    count=1\n",
    "    pre_prefix=0\n",
    "    for pos,col in enumerate(event_cat_col):\n",
    "        if x_data.loc[row,col] ==1:\n",
    "            row_train.append(vocab[col])\n",
    "            if pre_prefix != col.split('_')[1]:\n",
    "                pre_prefix=col.split('_')[1]\n",
    "                count =1\n",
    "            else:\n",
    "                count +=1\n",
    "    for pos,col in enumerate(event_con_col):\n",
    "        row_train.append(x_data.loc[row,col])\n",
    "    row_train = torch.tensor(row_train, dtype=torch.float)\n",
    "    nx_data.append(row_train)\n",
    "    \n",
    "x_train, x_test, y_train, y_test = train_test_split(nx_data, y_data, test_size=0.33, random_state=69)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = Customdataset(x_train,y_train)\n",
    "testset = Customdataset(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size =10\n",
    "train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(testset,batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_prediction(nn.Module):\n",
    "    def __init__(self,vocab_size, embedding_dim):\n",
    "        super(MLP_prediction,self).__init__()\n",
    "        \n",
    "        self.embeddings = nn.Embedding(len(vocab), embedding_dim)\n",
    "        self.linear1 = nn.Linear(5*50,5*50)\n",
    "        self.linear2 = nn.Linear(5*50, 1)\n",
    "        \n",
    "        # MLP part\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.relu = nn.LeakyReLU()\n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        implement code here\n",
    "        \"\"\"\n",
    "        l3 =[]\n",
    "        for case in inputs:\n",
    "            con = case[-10:]\n",
    "            cat = case[:-10].long()\n",
    "            l1 = self.embeddings(cat).view(-1,48)\n",
    "            rearrange=[]\n",
    "            for pos,k in enumerate(l1):  \n",
    "                t = torch.cat((k,con[pos*2:pos*2+2]))\n",
    "                rearrange.append(t)\n",
    "            l2 = torch.cat(rearrange).view(1,-1)\n",
    "            l3.append(l2)\n",
    "        hidden = torch.cat(l3)\n",
    "        hidden = self.linear1(hidden)\n",
    "        hidden = self.relu(hidden)\n",
    "        hidden = self.dropout(hidden)\n",
    "        hidden = self.linear2(hidden)\n",
    "        outputs = hidden.squeeze(1)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "embedding_dim=6\n",
    "model = MLP_prediction(len(vocab),embedding_dim = embedding_dim).cuda()\n",
    "\n",
    "# Loss function & Optimizers\n",
    "\"\"\"\n",
    "you can change the loss and optimizer\n",
    "\"\"\"\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0002)#, weight_decay=1e-4)\n",
    "\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "\"\"\"\n",
    "you can change the value\n",
    "\"\"\"\n",
    "num_epochs = 100\n",
    "batch_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_acc(train_predict, train_y):\n",
    "    train_predict_tag = torch.round(torch.sigmoid(train_predict))\n",
    "    correct_results_sum = (train_predict_tag == train_y).sum().float()\n",
    "    acc = correct_results_sum/train_y.shape[0]\n",
    "    acc = torch.round(acc *100)\n",
    "    \n",
    "    return acc    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train accuracy: 65.62 %, test accuracy: 69.77 %\n",
      "epoch:1, train_loss: 0.6234, test_loss: 0.5929\n",
      "Saving model\n",
      "\n",
      "train accuracy: 75.78 %, test accuracy: 71.06 %\n",
      "epoch:6, train_loss: 0.5111, test_loss: 0.5821\n",
      "Saving model\n",
      "\n",
      "train accuracy: 81.41 %, test accuracy: 70.74 %\n",
      "epoch:11, train_loss: 0.4402, test_loss: 0.5908\n",
      "\n",
      "train accuracy: 85.94 %, test accuracy: 68.49 %\n",
      "epoch:16, train_loss: 0.3645, test_loss: 0.6002\n",
      "\n",
      "train accuracy: 90.00 %, test accuracy: 66.56 %\n",
      "epoch:21, train_loss: 0.2892, test_loss: 0.6317\n",
      "\n",
      "train accuracy: 92.50 %, test accuracy: 68.81 %\n",
      "epoch:26, train_loss: 0.2383, test_loss: 0.6695\n",
      "\n",
      "train accuracy: 96.56 %, test accuracy: 67.20 %\n",
      "epoch:31, train_loss: 0.1811, test_loss: 0.7052\n",
      "\n",
      "train accuracy: 97.19 %, test accuracy: 66.56 %\n",
      "epoch:36, train_loss: 0.1500, test_loss: 0.7434\n",
      "\n",
      "train accuracy: 98.28 %, test accuracy: 68.17 %\n",
      "epoch:41, train_loss: 0.1199, test_loss: 0.7929\n",
      "\n",
      "train accuracy: 99.22 %, test accuracy: 68.17 %\n",
      "epoch:46, train_loss: 0.0901, test_loss: 0.8289\n",
      "\n",
      "train accuracy: 99.22 %, test accuracy: 68.81 %\n",
      "epoch:51, train_loss: 0.0696, test_loss: 0.8776\n",
      "\n",
      "train accuracy: 99.69 %, test accuracy: 68.17 %\n",
      "epoch:56, train_loss: 0.0570, test_loss: 0.9339\n",
      "\n",
      "train accuracy: 99.84 %, test accuracy: 69.13 %\n",
      "epoch:61, train_loss: 0.0461, test_loss: 0.9557\n",
      "\n",
      "train accuracy: 99.84 %, test accuracy: 66.24 %\n",
      "epoch:66, train_loss: 0.0362, test_loss: 1.0157\n",
      "\n",
      "train accuracy: 100.00 %, test accuracy: 66.88 %\n",
      "epoch:71, train_loss: 0.0310, test_loss: 1.0710\n",
      "\n",
      "train accuracy: 100.00 %, test accuracy: 66.56 %\n",
      "epoch:76, train_loss: 0.0251, test_loss: 1.0999\n",
      "\n",
      "train accuracy: 100.00 %, test accuracy: 68.81 %\n",
      "epoch:81, train_loss: 0.0205, test_loss: 1.1605\n",
      "\n",
      "train accuracy: 100.00 %, test accuracy: 67.20 %\n",
      "epoch:86, train_loss: 0.0158, test_loss: 1.1918\n",
      "\n",
      "train accuracy: 100.00 %, test accuracy: 68.49 %\n",
      "epoch:91, train_loss: 0.0142, test_loss: 1.2300\n",
      "\n",
      "train accuracy: 100.00 %, test accuracy: 68.17 %\n",
      "epoch:96, train_loss: 0.0148, test_loss: 1.2638\n"
     ]
    }
   ],
   "source": [
    "# Train CNN_prediction first\n",
    "\n",
    "accuracy_graph = {'train':[], 'test':[], 'epoch': []}\n",
    "loss_graph = {'train':[], 'test':[], 'epoch': []}\n",
    "# model = MLP_prediction().cuda()\n",
    "best_loss=10\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    # Training\n",
    "    for train_x, train_y in train_loader: \n",
    "#         train_y = train_y.squeeze(1)\n",
    "        train_x,train_y = train_x.cuda(), train_y.cuda()\n",
    "        train_predict = model(train_x)\n",
    "        train_predict = train_predict.float()\n",
    "        train_y = train_y.float()\n",
    "        loss = criterion(train_predict, train_y)\n",
    "        \n",
    "        acc = binary_acc(train_predict, train_y)\n",
    "\n",
    "        # Backpropagation        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "\n",
    "    # Evaluation\n",
    "    if epoch % 5 ==0:\n",
    "        \n",
    "        test_acc = 0\n",
    "        test_loss =0\n",
    "        \n",
    "        for test_x, test_y in test_loader:\n",
    "\n",
    "            with torch.autograd.no_grad():\n",
    "                test_x, test_y = test_x.cuda(), test_y.cuda()\n",
    "                test_predict = model(test_x)\n",
    "            test_predict = test_predict.float()\n",
    "            test_y = test_y.float()\n",
    "\n",
    "            loss = criterion(test_predict, test_y)\n",
    "            acc = binary_acc(test_predict, test_y)\n",
    "            \n",
    "            test_loss += loss.item()\n",
    "            test_acc += acc.item()\n",
    "                               \n",
    "        print(\"\\ntrain accuracy: {:.2f} %, test accuracy: {:.2f} %\".format(epoch_acc/len(train_loader), test_acc/len(test_loader)))\n",
    "        print(\"epoch:{}, train_loss: {:.4f}, test_loss: {:.4f}\".format(epoch+1, epoch_loss/len(train_loader), test_loss/len(test_loader))) \n",
    "        accuracy_graph['epoch'] = epoch+1\n",
    "        accuracy_graph['train'] = epoch_acc/len(train_loader)\n",
    "\n",
    "        loss_graph['epoch'].append(epoch+1)\n",
    "        loss_graph['train'].append(epoch_loss/len(train_loader))\n",
    "        loss_graph['test'].append(test_loss/len(test_loader))\n",
    "    \n",
    "        if test_loss/len(test_loader) < best_loss:\n",
    "            best_loss = test_loss/len(test_loader)\n",
    "            print('Saving model')\n",
    "            torch.save(model.state_dict(), \"./embed_predic.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.64      0.69       154\n",
      "           1       0.69      0.78      0.73       157\n",
      "\n",
      "    accuracy                           0.71       311\n",
      "   macro avg       0.72      0.71      0.71       311\n",
      "weighted avg       0.72      0.71      0.71       311\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_acc = 0\n",
    "test_loss =0\n",
    "y_predict_list=[]\n",
    "device = torch.device(\"cuda\")\n",
    "model = MLP_prediction(len(vocab),embedding_dim = embedding_dim).cuda()\n",
    "model.load_state_dict(torch.load(\"./embed_predic.pt\", map_location=\"cuda:0\"))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "testset = Customdataset(x_test,y_test)\n",
    "test_loader = DataLoader(testset,batch_size=1)\n",
    "\n",
    "with torch.autograd.no_grad():\n",
    "    for test_x, test_y in test_loader:\n",
    "        \n",
    "        test_x, test_y = test_x.cuda(), test_y.cuda()\n",
    "        test_predict = model(test_x)\n",
    "        test_predict = test_predict.float()\n",
    "        test_y = test_y.float()\n",
    "        \n",
    "        test_predict_tag = torch.round(torch.sigmoid(test_predict))\n",
    "        y_predict_list.append(test_predict_tag.cpu().numpy())\n",
    "        \n",
    "print(classification_report(y_test,y_predict_list))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
