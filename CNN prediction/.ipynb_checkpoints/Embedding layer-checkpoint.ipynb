{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying embedding layer\n",
    "\n",
    "Let's apply embedding layer before linear layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils as torch_utils\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader,Dataset,WeightedRandomSampler\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Aug  6 17:15:11 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 396.51                 Driver Version: 396.51                    |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce GTX 108...  Off  | 00000000:02:00.0 Off |                  N/A |\n",
      "| 26%   39C    P8    16W / 250W |   2405MiB / 11177MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  GeForce GTX 108...  Off  | 00000000:03:00.0 Off |                  N/A |\n",
      "| 23%   36C    P8    16W / 250W |     12MiB / 11178MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  GeForce GTX 108...  Off  | 00000000:82:00.0 Off |                  N/A |\n",
      "| 23%   37C    P8    17W / 250W |     12MiB / 11178MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  GeForce GTX 108...  Off  | 00000000:83:00.0 Off |                  N/A |\n",
      "| 23%   35C    P8    16W / 250W |     12MiB / 11178MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|    0      4118      C   python3                                      161MiB |\n",
      "|    0      5258      C   /opt/anaconda3/bin/python3                   159MiB |\n",
      "|    0     12117      C   python3                                      161MiB |\n",
      "|    0     16724      C   /opt/anaconda3/bin/python3                   155MiB |\n",
      "|    0     16727      C   /opt/anaconda3/bin/python3                   155MiB |\n",
      "|    0     16728      C   /opt/anaconda3/bin/python3                   155MiB |\n",
      "|    0     16731      C   /opt/anaconda3/bin/python3                   155MiB |\n",
      "|    0     16761      C   /opt/anaconda3/bin/python3                   155MiB |\n",
      "|    0     16765      C   /opt/anaconda3/bin/python3                   155MiB |\n",
      "|    0     16766      C   /opt/anaconda3/bin/python3                   155MiB |\n",
      "|    0     16780      C   /opt/anaconda3/bin/python3                   155MiB |\n",
      "|    0     16805      C   /opt/anaconda3/bin/python3                   155MiB |\n",
      "|    0     16821      C   /opt/anaconda3/bin/python3                   155MiB |\n",
      "|    0     16947      G   /usr/lib/xorg/Xorg                            47MiB |\n",
      "|    0     31689      C   /opt/anaconda3/bin/python3                   163MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct custom dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Customdataset(Dataset):\n",
    "    def __init__(self,x_data,y_data,transform=None):\n",
    "        '''\n",
    "        Call stored dataset\n",
    "        \n",
    "        Params\n",
    "        second: Ellapsed second from the beginning of events \n",
    "        encoding_type: Encoding method for outcomeprediction ex) Static, last_state, aggregation, etc.\n",
    "        '''\n",
    "        \n",
    "            \n",
    "        # Transforms\n",
    "        self.y_data=np.array(y_data)\n",
    "        self.x_data=x_data\n",
    "    def __len__(self):\n",
    "        return len(self.x_data)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        \n",
    "        # Convert x and y data to torch flaot tensor\n",
    "        x = self.x_data[idx]\n",
    "        y = self.y_data[idx]\n",
    "        return x,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To equalize size of each input data, delete columns that doesn't exist across all cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding_type = 'last_state'\n",
    "prefix = 5\n",
    "input_data = pd.read_csv('../data/'+encoding_type+'_'+str(prefix)+'.csv')\n",
    "\n",
    "deletecols=[]\n",
    "for cols in list(input_data.columns.values):\n",
    "    if 'IDofConceptCase' in cols or 'parts' in cols or 'landRegisterID' in cols:\n",
    "        deletecols.append(cols)\n",
    "\n",
    "input_data =input_data.drop(['Case ID', 'Duration from previous', 'Duration from start','(case) SUMleges']+deletecols,axis=1)\n",
    "input_data['sum'] = input_data.sum(axis=1)\n",
    "\n",
    "input_data = input_data[input_data['sum'].isin([15,16])].reset_index(drop=True)\n",
    "y_data = [int(y) for y in list(input_data['Label'])]\n",
    "input_data = input_data.drop(['Label'],axis=1)\n",
    "\n",
    "x_data = input_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.33, random_state=1)\n",
    "\n",
    "weekcols = []\n",
    "for x in x_data.columns.values:\n",
    "    weekcols.append(x)\n",
    "\n",
    "\n",
    "vocab = {word: i+1 for i, word in enumerate(weekcols)}\n",
    "dft = x_train.loc[:,weekcols].reset_index(drop=True)\n",
    "\n",
    "x_train=[]\n",
    "for row in range(len(dft)):\n",
    "    row_train=[]\n",
    "    for col in weekcols:\n",
    "        if dft.loc[row,col] ==1:\n",
    "            row_train.append(vocab[col])\n",
    "    row_train = torch.tensor(row_train, dtype=torch.long)\n",
    "    x_train.append(row_train)\n",
    "\n",
    "dft = x_test.loc[:,weekcols].reset_index(drop=True)\n",
    "x_test=[]\n",
    "for row in range(len(dft)):\n",
    "    row_test=[]\n",
    "    for col in weekcols:\n",
    "        if dft.loc[row,col] ==1:\n",
    "            row_test.append(vocab[col])\n",
    "    row_test = torch.tensor(row_test, dtype=torch.long)\n",
    "    x_test.append(row_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainset = Customdataset(x_train,y_train)\n",
    "testset = Customdataset(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size =1\n",
    "train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(testset,batch_size=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_prediction(nn.Module):\n",
    "    def __init__(self,vocab_size, embedding_dim,atts_number):\n",
    "        super(MLP_prediction,self).__init__()\n",
    "        \n",
    "        self.embeddings = nn.Embedding(vocab_size+1, embedding_dim)\n",
    "        self.linear1 = nn.Linear(atts_number * embedding_dim, atts_number * embedding_dim)\n",
    "        self.linear2 = nn.Linear(atts_number * embedding_dim, 1)\n",
    "#         self.linear3 = nn.Linear(32, 1)\n",
    "        \n",
    "        # MLP part\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.relu = nn.LeakyReLU()\n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "    \n",
    "    def forward(self, inputs):\n",
    "\n",
    "        \"\"\"\n",
    "        implement code here\n",
    "        \"\"\"\n",
    "        hidden = self.embeddings(inputs).view(1,-1)\n",
    "        hidden = self.relu(self.linear1(hidden))\n",
    "        hidden = self.dropout(hidden)\n",
    "#         hidden = self.relu(self.linear2(hidden))       \n",
    "        hidden = self.linear2(hidden)\n",
    "#         hidden = self.sig(hidden)\n",
    "        outputs = hidden.squeeze(1)\n",
    "        return outputs\n",
    "\n",
    "atts_number = len(x_train[0])\n",
    "embedding_dim=3\n",
    "model = MLP_prediction(len(vocab),embedding_dim = embedding_dim,atts_number = atts_number).cuda()\n",
    "\n",
    "# Loss function & Optimizers\n",
    "\"\"\"\n",
    "you can change the loss and optimizer\n",
    "\"\"\"\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)#, weight_decay=1e-4)\n",
    "\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "\"\"\"\n",
    "you can change the value\n",
    "\"\"\"\n",
    "num_epochs = 30\n",
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_acc(train_predict, train_y):\n",
    "    train_predict_tag = torch.round(torch.sigmoid(train_predict))\n",
    "    correct_results_sum = (train_predict_tag == train_y).sum().float()\n",
    "    acc = correct_results_sum/train_y.shape[0]\n",
    "    acc = torch.round(acc *100)\n",
    "    \n",
    "    return acc    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train accuracy: 80.51 %, test accuracy: 95.99 %\n",
      "epoch:1, train_loss: 0.4642, test_loss: 0.2289\n",
      "\n",
      "train accuracy: 96.39 %, test accuracy: 98.18 %\n",
      "epoch:6, train_loss: 0.1033, test_loss: 0.0807\n",
      "\n",
      "train accuracy: 96.93 %, test accuracy: 97.45 %\n",
      "epoch:11, train_loss: 0.0767, test_loss: 0.0830\n",
      "\n",
      "train accuracy: 97.65 %, test accuracy: 97.45 %\n",
      "epoch:16, train_loss: 0.0553, test_loss: 0.0985\n",
      "\n",
      "train accuracy: 98.92 %, test accuracy: 97.45 %\n",
      "epoch:21, train_loss: 0.0394, test_loss: 0.1009\n",
      "\n",
      "train accuracy: 99.28 %, test accuracy: 97.08 %\n",
      "epoch:26, train_loss: 0.0247, test_loss: 0.1170\n"
     ]
    }
   ],
   "source": [
    "# Train CNN_prediction first\n",
    "\n",
    "accuracy_graph = {'train':[], 'test':[], 'epoch': []}\n",
    "loss_graph = {'train':[], 'test':[], 'epoch': []}\n",
    "# model = MLP_prediction().cuda()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    # Training\n",
    "    for train_x, train_y in train_loader: \n",
    "#         train_y = train_y.squeeze(1)\n",
    "        train_x,train_y = train_x.cuda(), train_y.cuda()\n",
    "        train_predict = model(train_x)\n",
    "        train_predict = train_predict.float()\n",
    "        train_y = train_y.float()\n",
    "        loss = criterion(train_predict, train_y)\n",
    "        \n",
    "        acc = binary_acc(train_predict, train_y)\n",
    "\n",
    "        # Backpropagation        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "\n",
    "    # Evaluation\n",
    "    if epoch % 5 ==0:\n",
    "        \n",
    "        test_acc = 0\n",
    "        test_loss =0\n",
    "        \n",
    "        for test_x, test_y in test_loader:\n",
    "\n",
    "            with torch.autograd.no_grad():\n",
    "#                 test_y = test_y.squeeze(1)\n",
    "                test_x, test_y = test_x.cuda(), test_y.cuda()\n",
    "                test_predict = model(test_x)\n",
    "            test_predict = test_predict.float()\n",
    "            test_y = test_y.float()\n",
    "\n",
    "            loss = criterion(test_predict, test_y)\n",
    "            acc = binary_acc(test_predict, test_y)\n",
    "            \n",
    "            test_loss += loss.item()\n",
    "            test_acc += acc.item()\n",
    "                               \n",
    "        print(\"\\ntrain accuracy: {:.2f} %, test accuracy: {:.2f} %\".format(epoch_acc/len(train_loader), test_acc/len(test_loader)))\n",
    "        print(\"epoch:{}, train_loss: {:.4f}, test_loss: {:.4f}\".format(epoch+1, epoch_loss/len(train_loader), test_loss/len(test_loader))) \n",
    "        accuracy_graph['epoch'] = epoch+1\n",
    "        accuracy_graph['train'] = epoch_acc/len(train_loader)\n",
    "\n",
    "        loss_graph['epoch'] = epoch+1\n",
    "        loss_graph['train'] = epoch_loss/len(train_loader)\n",
    "        loss_graph['test'] = test_loss/len(test_loader)\n",
    "\n",
    "\n",
    "#         torch.save(model.state_dict(), \"./cnn_predic.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.97      0.98       233\n",
      "           1       0.87      0.95      0.91        41\n",
      "\n",
      "    accuracy                           0.97       274\n",
      "   macro avg       0.93      0.96      0.94       274\n",
      "weighted avg       0.97      0.97      0.97       274\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_acc = 0\n",
    "test_loss =0\n",
    "y_predict_list=[]\n",
    "model.eval()\n",
    "\n",
    "testset = Customdataset(x_test,y_test)\n",
    "test_loader = DataLoader(testset,batch_size=1)\n",
    "\n",
    "with torch.autograd.no_grad():\n",
    "    for test_x, test_y in test_loader:\n",
    "        \n",
    "        test_x, test_y = test_x.cuda(), test_y.cuda()\n",
    "        test_predict = model(test_x)\n",
    "        test_predict = test_predict.float()\n",
    "        test_y = test_y.float()\n",
    "        \n",
    "        test_predict_tag = torch.round(torch.sigmoid(test_predict))\n",
    "        y_predict_list.append(test_predict_tag.cpu().numpy())\n",
    "        \n",
    "print(classification_report(y_test,y_predict_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
